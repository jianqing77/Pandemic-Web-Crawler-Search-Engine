{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.sessions import Session\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import PriorityQueue\n",
    "from urllib.parse import urlparse, urljoin, urlunparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import re\n",
    "import os.path\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "# import threading\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from urllib.error import URLError\n",
    "from requests.exceptions import ConnectTimeout, ReadTimeout, RequestException\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration & Variable Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMON_URLS = ['https://en.wikipedia.org/wiki/List_of_epidemics',\n",
    "               'https://www.livescience.com/worst-epidemics-and-pandemics-in-history.html',\n",
    "               'https://en.wikipedia.org/wiki/Pandemic']\n",
    "\n",
    "UNIQUE_URLS = ['https://www.cdc.gov/vhf/ebola/history/2014-2016-outbreak/index.html',\n",
    "               'https://www.idsociety.org/public-health/ebola/ebola-resources/ebola-facts/',\n",
    "               'https://preventepidemics.org/epidemics-that-didnt-happen/ebola/']\n",
    "\n",
    "SEEDS_URLS = COMMON_URLS + UNIQUE_URLS\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory                 \n",
    "PATH_SCRIPT = os.path.abspath(cwd) \n",
    "PATH_DIR_RESULTS = os.path.join(PATH_SCRIPT, '..', 'Results', )\n",
    "PATH_DIR_CRAWLED_DATA = os.path.join(PATH_DIR_RESULTS, \"data\")\n",
    "PATH_INLINKS = os.path.join(PATH_DIR_RESULTS, 'links', 'inlinks.json')\n",
    "PATH_OUTLINKS = os.path.join(PATH_DIR_RESULTS, 'links', 'outlinks.json')\n",
    "PATH_INLINKS_COUNTS = os.path.join(PATH_DIR_RESULTS, 'links', 'inlinks_counts.json')\n",
    "\n",
    "KEYWORDS = [\"pandemic\", \"pandemics\", \"epidemic\", \"epidemics\", \"evd\", \"ebola\", \"africa\", \"african\", \"congo\", \"west\", \"disease\", \"death\", \"infection\", \"illness\", \"transmission\", \"dead\", \"virus\", \"outbreak\"]\n",
    "LIMIT = 30000\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# config logging\n",
    "logging.basicConfig(\n",
    "    filename='main.log',  \n",
    "    filemode='a',       # append mode    \n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "socket.setdefaulttimeout(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frontier = PriorityQueue() # Frontier: priority queue => store tuples in format (priority_score, url, wave_number, raw_content)\n",
    "inlinks_counts = {}  # Inlink Counts: dictionary => key: url, value: the inlink counts (to be easily updated)\n",
    "inlinks_dict = {} # Inlinks: dictionary => key: url, value: urls that derived from the key url\n",
    "outlinks_dict = {} # Outlinks: dictionary => key: url, value: the outlinks of current url\n",
    "\n",
    "# count = 1 # Count the # of pages crawled \n",
    "\n",
    "frontier_urls = set()  # after being added to frontier => unique\n",
    "visited_urls = set()  # after crawling the outlinks and write to files  -- final length = 30,000\n",
    "bad_urls = set()      # mark links that have been examined as invalid\n",
    "\n",
    "domain_delays = {} # dictionary to store the crawl delay and the time of the last request for each domain\n",
    "last_request_time = {} # dict to keep track of the last request time for each domain\n",
    "\n",
    "\n",
    "BLACK_LIST_EXTENSIONS = (\".jpg\", \".svg\", \".png\", \".pdf\", \".gif\")\n",
    "BLACK_LIST_DOMAINS = (\"youtube\", \"amazon\", \"google\", \"yahoo\", \"bing\", \"facebook\", \"twitter\")\n",
    "MIN_ABS_SCORE = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. URL: Canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Canonicalizes a given URL according to specific rules:\n",
    "\"\"\"\n",
    "def canonicalize_url(url):\n",
    "    try:\n",
    "        # use the urlparse to parse the URL\n",
    "        parse_res = urlparse(url) # ParseResult(scheme='https', netloc='www.mphonline.org', path='/worst-pandemics-in-history/', params='', query='', fragment='')\n",
    "\n",
    "        # 1. Convert scheme and host to lower case\n",
    "        scheme = parse_res.scheme.lower()\n",
    "        netloc = parse_res.netloc.lower()\n",
    "\n",
    "        # 2. Remove default port 80 for HTTP and 443 for HTTPS\n",
    "        if scheme == 'http' and parse_res.port == 80:\n",
    "            netloc = parse_res.hostname\n",
    "        elif scheme == 'https' and parse_res.port == 443:\n",
    "            netloc = parse_res.hostname\n",
    "\n",
    "        path = parse_res.path\n",
    "        # 3. Make relative URLs absolute\n",
    "        if not path.startswith('/'):\n",
    "            path = '/' + path\n",
    "\n",
    "        # 4. Remove the fragment: make the fragment empty\n",
    "        fragment = ''\n",
    "\n",
    "        # 5. Remove duplicate slashes\n",
    "        path = path.replace('//', '/')\n",
    "\n",
    "        # use urlunparse to reconstruct the URL\n",
    "        canonicalized_url = urlunparse((scheme, netloc, path, parse_res.params, parse_res.query, fragment))\n",
    "        return canonicalized_url\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during URL canonicalization: {e}\")\n",
    "        canonicalized_url = url\n",
    "\n",
    "    return canonicalized_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Politeness Policy: Crawl Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper method to get the crawl delay from robots.txt\n",
    "\"\"\"\n",
    "def get_crawl_delay(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    \n",
    "    try:\n",
    "        rp.read()\n",
    "    except UnicodeDecodeError as e:\n",
    "        # logging.error(f\"Failed to decode robots.txt for {url}: {e}\")\n",
    "        return 1 \n",
    "    except URLError as e:\n",
    "        # logging.error(f\"URLError when accessing robots.txt for {url}: {e}\")\n",
    "        return 1 \n",
    "    except Exception as e:\n",
    "        # logging.error(f\"An unexpected error occurred when accessing robots.txt for {url}: {e}\")\n",
    "        return 1  \n",
    "    \n",
    "    # use RobotFileParser to get the 'Crawl-Delay' for the user-agent '*'\n",
    "    crawl_delay = rp.crawl_delay(\"*\")\n",
    "    if crawl_delay is None:\n",
    "        crawl_delay = 1  # if not present, default to one request per second\n",
    "    \n",
    "    return crawl_delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper method to enforce delay between requests to the same domain.\n",
    "It ensures that there is a minimum delay between subsequent requests to the same domain to avoid overloading the server \n",
    "and to comply with the crawl delay specified by the website.\n",
    "\"\"\"\n",
    "\n",
    "def enforce_delay(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    current_time = time.time()\n",
    "    sleep_time = 0\n",
    "\n",
    "    if domain not in domain_delays:\n",
    "        crawl_delay = get_crawl_delay(url)\n",
    "        domain_delays[domain] = {'crawl_delay': crawl_delay, 'last_request_time': current_time}\n",
    "    else:\n",
    "        last_request_time = domain_delays[domain]['last_request_time']\n",
    "        crawl_delay = domain_delays[domain]['crawl_delay']\n",
    "        time_since_last_request = current_time - last_request_time\n",
    "\n",
    "        if time_since_last_request < crawl_delay:\n",
    "            sleep_time = crawl_delay - time_since_last_request\n",
    "\n",
    "        domain_delays[domain]['last_request_time'] = current_time + sleep_time\n",
    "\n",
    "    if sleep_time > 0:\n",
    "        try:\n",
    "            time.sleep(sleep_time)\n",
    "        except KeyboardInterrupt:\n",
    "            # Handle the interrupt, possibly by saving state or cleaning up resources\n",
    "            print(f\"Interrupted during sleep: {sleep_time}s delay was enforced for domain {domain}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. URL: Qualification Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Check if the URL uses HTTP or HTTPS protocol.\n",
    "\"\"\"\n",
    "def is_http_or_https(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.scheme not in ['http', 'https']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\"\"\"Determine if the given URL is blacklisted based on its domain and file extension\n",
    "\"\"\"\n",
    "def is_blacklisted(url):\n",
    "    if url.lower().endswith(BLACK_LIST_EXTENSIONS):\n",
    "        return True\n",
    "    \n",
    "    domain_pattern = r\"https?://(www\\.)?({})\".format(\"|\".join(BLACK_LIST_DOMAINS))\n",
    "    if re.search(domain_pattern, url.lower()):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\"\"\"Check the Content-Type and language of the URL's response header.\n",
    "\"\"\"\n",
    "def is_ideal_type(header_response):\n",
    "\n",
    "    content_type = header_response.get('Content-Type', '')\n",
    "    content_language = header_response.get('Content-Language', '')\n",
    "    \n",
    "    # Check if the response Content-Type is 'text/html'\n",
    "    is_html = 'text/html' in content_type\n",
    "    \n",
    "    # Check if the Content-Language is English or not specified\n",
    "    is_english = ('en' in content_language or content_language == '')\n",
    "\n",
    "    return is_html and is_english\n",
    "\n",
    "    \n",
    "\"\"\"Use RobotFileParser to check if crawling the URL is allowed.\n",
    "\"\"\"\n",
    "def is_allowed_by_robots(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    \n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch('*', url) \n",
    "    except Exception:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "--------------------------------------------------------\n",
    "STAGE 2: PRIORITY SCORE CALCULATION\n",
    "\n",
    "### 1. Inlinks and Outlinks Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_inlink_count(url):\n",
    "    if url in inlinks_counts:               \n",
    "        inlinks_counts[url] += 1           # if the URL is already in the dictionary(duplicate), increment the inlink count \n",
    "    else:\n",
    "        inlinks_counts[url] = 0           # If not, initialize it with a count of 1\n",
    "\n",
    "\n",
    "def update_inlink_dict(url, source_url):\n",
    "    if url in inlinks_dict:\n",
    "        inlinks_dict[url].add(source_url)\n",
    "    else:\n",
    "        inlinks_dict[url] = {source_url}\n",
    "\n",
    "\n",
    "def dump_links(links_dict, filename):\n",
    "    json_ready_dict = {}\n",
    "\n",
    "    for key, value in links_dict.items():\n",
    "        # convert any sets to lists\n",
    "        if isinstance(value, set):\n",
    "            json_ready_dict[key] = list(value)\n",
    "        else:\n",
    "            json_ready_dict[key] = value\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(json_ready_dict, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Content\n",
    "- title, anchor text => priority score calculation\n",
    "- out-links => analyzing outlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    enforce_delay(url)  \n",
    "    try:\n",
    "        if not is_allowed_by_robots(url):\n",
    "            bad_urls.add(url)\n",
    "            return None, None\n",
    "        response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        response.raise_for_status() \n",
    "        if response.status_code == 200:\n",
    "            return response.text, response.headers  \n",
    "        else:\n",
    "            logging.info(f\"Skipped: status code not 200.\")\n",
    "            return None, None\n",
    "    except socket.timeout:\n",
    "        logging.error(f\"Socket operation timed out for URL: {url}\")\n",
    "        return None, None\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Request failed: fetch_raw_html_content -- {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Skipped. An error occurred while fetching content from {url}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_title_and_anchor_texts(raw_html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html_content, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title_tag = soup.find('title')\n",
    "        page_title = title_tag.get_text(strip=True) if title_tag else \"No title found\"\n",
    "        \n",
    "        # Extract anchor texts\n",
    "        anchor_texts = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            text = link.get_text(strip=True)\n",
    "            if text:  \n",
    "                anchor_texts.append(text)\n",
    "\n",
    "        return page_title, anchor_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        # logging.exception(f\"An error occurred in extract_title_and_anchor_texts while extracting from {url}: {e}\")\n",
    "        return \"No title found\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_outlinks(url, raw_html_content):\n",
    "    \n",
    "    soup = BeautifulSoup(raw_html_content, 'html.parser')\n",
    "    \n",
    "    # extract outlinks within the raw html content\n",
    "    outlinks = [] \n",
    "\n",
    "    for link in soup.find_all('a', href=True): # use BeautifulSoup find_all method which searches the soup object for all HTML a elements that have an href attribute.\n",
    "        absolute_link = urljoin(url, link['href'])\n",
    "        outlinks.append(absolute_link)\n",
    "    \n",
    "    return outlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. URL Priority Score Calculation & Adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality: wave number, domain, in-links count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_domain_score(url, weight):\n",
    "    domain_scores = {'.gov': 5, '.edu': 4, '.org': 3}     # assign scores based on domain extension\n",
    "    default_score = 1                                                           # default score for unlisted domain extensions\n",
    "    parsed_url = urlparse(url)                                                  # get domain extension\n",
    "    domain_extension = ''.join(parsed_url.netloc.split('.')[-1:])\n",
    "    # print(f'Domain for:{url}: {domain_extension}')\n",
    "    return domain_scores.get('.' + domain_extension, default_score)  * weight    # use the domain extension to get the score, default to 1 if not found\n",
    "\n",
    "\n",
    "\n",
    "def cal_wave_score(wave_number, base_score=1000, decay_rate=0.9):\n",
    "    wave_number = max(wave_number, 0)  # Ensure the wave number is a positive value\n",
    "    score = base_score * math.pow(decay_rate, wave_number)  # Calculate the wave score using exponential decay\n",
    "    return max(score, 1)  # Ensure the weighted score does not drop below a certain threshold (e.g., 1)\n",
    "\n",
    "\n",
    "def cal_inlink_score(inlink_count, weight):\n",
    "    return (math.log(inlink_count + 1) if inlink_count > 0 else 0) * weight\n",
    "\n",
    "\n",
    "def cal_quality_score(url, inlink_count, qual_weight):\n",
    "    \n",
    "    # wave_weight = 0.5\n",
    "    domain_weight = 0.5\n",
    "    inlink_weight = 0.5\n",
    "    \n",
    "    # wave_score = cal_wave_score(wave_number, wave_weight) \n",
    "    domain_score = cal_domain_score(url, domain_weight) \n",
    "    inlink_score = cal_inlink_score(inlink_count, inlink_weight)\n",
    "    # print(f'wave_score:{wave_score}\\ndomain_score: {domain_score}\\ninlink_score: {inlink_score}')\n",
    "\n",
    "        \n",
    "    return (domain_score + inlink_score) * qual_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance: url, title, anchor text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Relevance: url keyword, title keyword, anchor text keyword\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def cal_keyword_score(text, weight):\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    text = text.lower()                                                  # normalize the text to lower case\n",
    "    keyword_count = sum(text.count(keyword) for keyword in KEYWORDS)     # count occurrences of each keyword\n",
    "    return keyword_count * weight                                        # multiply by weight, might want to normalize by text length\n",
    "\n",
    "\n",
    "def cal_anchor_text_score(anchor_texts, weight):\n",
    "    score = 0\n",
    "    total_anchor_text_length = sum(len(anchor_text.split()) for anchor_text in anchor_texts if anchor_text.strip())\n",
    "\n",
    "    for anchor_text in anchor_texts:\n",
    "        anchor_text = anchor_text.strip()\n",
    "        if anchor_text:\n",
    "            normalized_anchor_text = anchor_text.lower()\n",
    "            keyword_count = sum(normalized_anchor_text.count(keyword) for keyword in KEYWORDS)\n",
    "            anchor_text_length = len(anchor_text.split())\n",
    "            keyword_density = keyword_count / anchor_text_length if anchor_text_length else 0             # Calculate the density of keywords in this anchor text\n",
    "            score += keyword_density\n",
    "\n",
    "    average_density_score = (score / len(anchor_texts)) if anchor_texts else 0     # If there are anchor texts, calculate the average keyword density score\n",
    "    normalized_score = average_density_score * weight\n",
    "\n",
    "    return normalized_score\n",
    "\n",
    "def cal_relevance_score(url, title, anchor_texts, rele_weight):\n",
    "    url_keyword_weight = 0.4\n",
    "    title_keyword_weight = 0.4\n",
    "    anchor_texts_weight = 0.3\n",
    "    # get url keyword score\n",
    "    url_keyword_score = cal_keyword_score(url, url_keyword_weight)\n",
    "    # get title keyword score\n",
    "    if title == \"No title found\" or not title:\n",
    "        title_keyword_score = 0\n",
    "    else:\n",
    "        title_keyword_score = cal_keyword_score(title, title_keyword_weight)\n",
    "    # get anchor text score\n",
    "    anchor_texts_score = cal_anchor_text_score(anchor_texts, anchor_texts_weight)\n",
    "    \n",
    "    # print(f'url_keyword_score:{url_keyword_score}\\ntitle_keyword_score: {title_keyword_score}\\nanchor_texts_score: {anchor_texts_score}')\n",
    "    return (url_keyword_score \n",
    "            + title_keyword_score\n",
    "            + anchor_texts_score) * rele_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def priority_score(url, wave_number, inlink_count, \n",
    "                   title, anchor_texts):\n",
    "    \n",
    "    qual_weight = 0.2\n",
    "    rele_weight = 0.8\n",
    "    \n",
    "    wave_score = cal_wave_score(wave_number) \n",
    "    qual_score = cal_quality_score(url, inlink_count, qual_weight)\n",
    "    rele_score = cal_relevance_score(url, title, anchor_texts, rele_weight)\n",
    "    # print(f'{url}: quality: {qual_score}, relevance: {rele_score}\\n')\n",
    "    result = wave_score + (qual_score + rele_score) * 10\n",
    "    # logging.info(f\"Score calculation DONE: {result} - wave_score: {wave_score} -- wave: {wave_number}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_absolute_score(prior_score, wave_number):\n",
    "    wave_score = cal_wave_score(wave_number)\n",
    "    abs_score = prior_score - wave_score\n",
    "    return abs_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "STAGE 3: ADD TO FRONTIER\n",
    "\n",
    "### Adding method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_url(url, priority_score, wave_number, text_response):\n",
    "    frontier.put((-priority_score, url, wave_number, text_response))\n",
    "    frontier_urls.add(url)\n",
    "    logging.info(f\"Frontier Progress: {len(frontier_urls)}/{LIMIT} - score: {priority_score}, wave: {wave_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "STAGE 4: DOCUMENT PROCESSING\n",
    "### Write Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define punctuation characters to be removed\n",
    "    punctuation_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "    # Remove punctuation from the text\n",
    "    return punctuation_pattern.sub('', text)\n",
    "\n",
    "def get_title_and_text(raw_html_content):\n",
    "    try:        \n",
    "        if raw_html_content is None:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(raw_html_content, 'html.parser')\n",
    "        \n",
    "        # get the title\n",
    "        title_tag = soup.find('title')\n",
    "        page_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "        \n",
    "        # remove punctuation from the title\n",
    "        page_title = remove_punctuation(page_title)\n",
    "        \n",
    "        # extract the main text and remove punctuation from paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        # english_paragraphs = [remove_punctuation(p.get_text(strip=True)) for p in paragraphs]\n",
    "        stripped = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        text = ' '.join(stripped)\n",
    "\n",
    "        return page_title, text\n",
    "\n",
    "    except Exception as e:\n",
    "        # logging.exception(f\"An error occurred while extracting data from {url}: {e}\")\n",
    "        return \"\", \"\"\n",
    "    \n",
    "\n",
    "def formatter(canon_url, title, text):    \n",
    "    # Use the provided canon_url instead of the one from the article object\n",
    "    docno = canon_url\n",
    "    \n",
    "    # Format the content\n",
    "    content = f'<DOC>\\n<DOCNO>{docno}</DOCNO>\\n'\n",
    "    content += f'<HEAD>{title if title else \"\"}</HEAD>\\n'\n",
    "    content += f'<TEXT>{text if text else \"\"}</TEXT>\\n</DOC>\\n'\n",
    "    \n",
    "    return content\n",
    "\n",
    "def write_batch_to_file(batch_content, batch_num):\n",
    "    batch_filename = f'wb-data-{batch_num}.txt'\n",
    "    file_path = os.path.join(PATH_DIR_CRAWLED_DATA, batch_filename)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8', errors='replace') as file:\n",
    "            file.write(batch_content)\n",
    "        logging.info(f\"Batch {batch_num} written to {file_path}\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"IOError in write_batch_to_file for batch {batch_num}: {e}\", exc_info=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINED HELPER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_qualified(url, header_response):\n",
    "    \n",
    "    if url in bad_urls:\n",
    "        logging.info(f\"SKIPPED -- examined as BAD URL: {url}\")\n",
    "        return False\n",
    "\n",
    "    if not is_http_or_https(url):\n",
    "        logging.info(f\"SKIPPED -- not an HTTP or HTTPS URL: {url}\")\n",
    "        bad_urls.add(url)\n",
    "        return False\n",
    "    \n",
    "    if is_blacklisted(url):\n",
    "        logging.info(f\"SKIPPED -- URL is blacklisted: {url}\")\n",
    "        bad_urls.add(url)\n",
    "        return False\n",
    "\n",
    "    if not is_ideal_type(header_response):\n",
    "        logging.info(f\"SKIPPED -- not ideal type for URL: {url}\")\n",
    "        bad_urls.add(url)\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Seeds URL Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seeds_initialization(seeds):\n",
    "    wave_num = 0\n",
    "    logging.info(\"======================= SEEDS INITIALIZATION STARTED ========================= \")\n",
    "    canon_seeds = [canonicalize_url(seed) for seed in seeds]\n",
    "\n",
    "    for canon_seed in canon_seeds:\n",
    "        text_response, header_response = get_response(canon_seed)\n",
    "        inlinks_dict[canon_seed] = set()\n",
    "        update_inlink_count(canon_seed)\n",
    "        inlink_count = inlinks_counts.get(canon_seed)\n",
    "        title, anchor_texts = extract_title_and_anchor_texts(text_response)\n",
    "        # ----- priority score calculation\n",
    "        score = priority_score(canon_seed, wave_num, inlink_count, title, anchor_texts)\n",
    "        text_response, _ = get_response(canon_seed)\n",
    "        add_url(canon_seed, score, wave_num, text_response)\n",
    "    logging.info(\"========================= SEEDS INITIALIZATION DONE =========================\\n \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Crawl Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document_data(url, batch_content, text_response):\n",
    "    # logging.info(f\"---- DOCUMENT DATA PROCESSING: {url}\")\n",
    "    try:\n",
    "        title, text = get_title_and_text(text_response)\n",
    "        batch_content += formatter(url, title, text)\n",
    "        logging.info(f\"Processed document data for: {url}\")\n",
    "        return batch_content, True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception while processing document data for URL {url}: {e}\", exc_info=True)\n",
    "        return batch_content, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crawl_main(seed_urls):\n",
    "    \n",
    "    # seeds initialization: add all seeds to frontier\n",
    "    seeds_initialization(seed_urls)\n",
    "    \n",
    "    batch_num = 1\n",
    "    batch_content = \"\"\n",
    "    count = 0  # Initialize count\n",
    "    reach_limit = False\n",
    "\n",
    "    while not frontier.empty():\n",
    "        \n",
    "        prior_score, popped_url, wave_number, raw_html_content = frontier.get()\n",
    "        logging.info(f\"POPPED: {popped_url}\")\n",
    "\n",
    "        # all frontier url's has valid html content\n",
    "\n",
    "        # for each popped url\n",
    "        # 1. save outlinks\n",
    "        outlinks = extract_outlinks(popped_url, raw_html_content)\n",
    "        total_outlinks = len(outlinks)\n",
    "        if outlinks:\n",
    "            # canonicalize\n",
    "            canon_outlinks = [canonicalize_url(outlink) for outlink in outlinks]\n",
    "            outlinks_dict[popped_url] = set(canon_outlinks)\n",
    "\n",
    "            if not reach_limit:\n",
    "                # if not reach limit, check if could add to frontier\n",
    "                for index, canon_outlink in enumerate(canon_outlinks, start=1):  # Start enumeration at 1\n",
    "                    logging.info(f\"------------------------------------------------------------------------------\")  \n",
    "                    logging.info(f\"Analyzing outlink {index}/{total_outlinks} {canon_outlink}\")  \n",
    "                \n",
    "                    # if in  bad_url\n",
    "                    if canon_outlink in bad_urls:\n",
    "                        continue\n",
    "\n",
    "                    # if already in frontier_urls    \n",
    "                    if canon_outlink in frontier_urls:\n",
    "                        update_inlink_count(canon_outlink)\n",
    "                        update_inlink_dict(canon_outlink, popped_url)\n",
    "                        continue\n",
    "\n",
    "                    text_response, header_response = get_response(canon_outlink)\n",
    "                    # server no repsonse\n",
    "                    if text_response is None or header_response is None:\n",
    "                        continue\n",
    "\n",
    "                    # not qualified\n",
    "                    if not is_qualified(canon_outlink, header_response):\n",
    "                        continue\n",
    "\n",
    "                    ol_wave_num = wave_number + 1\n",
    "                    \n",
    "                    logging.info(f\"qualified\")\n",
    "\n",
    "                    # ----- priority score calculation preparation\n",
    "                    update_inlink_count(canon_outlink)\n",
    "                    update_inlink_dict(canon_outlink, popped_url)\n",
    "\n",
    "                    # Check if the length of frontier_urls has reached or exceeded the LIMIT\n",
    "                    inlink_count = inlinks_counts.get(canon_outlink)\n",
    "                    title, anchor_texts = extract_title_and_anchor_texts(text_response)\n",
    "                    # ----- priority score calculation\n",
    "                    score = priority_score(canon_outlink, ol_wave_num, inlink_count, title, anchor_texts)\n",
    "                    abs_score = get_absolute_score(score, ol_wave_num)\n",
    "                    # ----- priority score check\n",
    "                    if abs_score >= MIN_ABS_SCORE: \n",
    "                        add_url(canon_outlink, score, ol_wave_num, text_response)  \n",
    "                        if len(frontier_urls) >= LIMIT:\n",
    "                            logging.info(f\"Frontier length {len(frontier_urls)} limit reached {LIMIT}. Stopping outlink processing.\")\n",
    "                            reach_limit = True\n",
    "                            break\n",
    "                    else:\n",
    "                        logging.info(f\"Score Low skipped - abs score({abs_score}) < threshold\")\n",
    "                        bad_urls.add(canon_outlink)\n",
    "                        continue\n",
    "\n",
    "        else:\n",
    "            outlinks_dict[popped_url] = set()\n",
    "\n",
    "\n",
    "        # 2. save to doc\n",
    "        logging.info(f\"================================================================================================\")\n",
    "        logging.info(f\"2. DOCUMENT DATA PROCESSING STARTED {popped_url}\")\n",
    "        batch_content, success = process_document_data(popped_url, batch_content, raw_html_content)\n",
    "        if success:\n",
    "            count += 1\n",
    "            visited_urls.add(popped_url)\n",
    "            logging.info(f\"===== FINISH ANALYSIS OF No.{count} in the frontier: {popped_url}\")\n",
    "\n",
    "            # Write to file if the batch is complete\n",
    "            if count % BATCH_SIZE == 0:\n",
    "                logging.info(f\"Writing batch content to file....\")\n",
    "                write_batch_to_file(batch_content, batch_num)\n",
    "                batch_num += 1\n",
    "                batch_content = \"\"\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_main(SEEDS_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dump_links(outlinks_dict, PATH_OUTLINKS)\n",
    "dump_links(inlinks_dict, PATH_INLINKS)\n",
    "dump_links(inlinks_counts, PATH_INLINKS_COUNTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
