{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde87a8b-dbbc-436f-bda3-32ad4e617049",
   "metadata": {},
   "source": [
    "#  Relevance Assessments, IR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0d8f7f36-b08a-48ba-9f03-26d513b7e3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import os.path  \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39623c-5fe6-490e-b5b6-f6b03d6e2e00",
   "metadata": {},
   "source": [
    "##  Assessment graphical interface & Manual assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2e80c-06aa-4f69-95df-5afb28bad929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f4bf9-a93b-4bf0-afdd-ebecd85e184f",
   "metadata": {},
   "source": [
    "## Trec_eval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f68a3b7-35f7-4d43-90de-6962bc297e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hold the final relevant docs\n",
    "# key: query id\n",
    "# value:  a list of doc_id that are considered relevant or strongly relevant based on their average relevance score\n",
    "rele_assess_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c2f72-85b5-4bd1-8b0c-c291e372cf4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Methods: parse the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7dcd50b9-1421-4dc6-bea3-32b010f89499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parses the ranked list file and returns an OrderedDict\n",
    "The DocIDs are listed in the order they appear in the file, which should be sorted by score already before writing the file. \n",
    "- key: QueryID\n",
    "- value: a list of DocIDs associated with that QueryID. => sorted by score\n",
    "\"\"\"\n",
    "def retrieve_query_results(query_result_file):\n",
    "    query_results = OrderedDict()\n",
    "    with open(query_result_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            query_id, _, doc_id, *rest = line.strip().split() # get only the query_id and doc_id\n",
    "            if query_id in query_results:\n",
    "                query_results[query_id].append(doc_id) # \n",
    "            else:\n",
    "                query_results[query_id] = [doc_id]\n",
    "    return query_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f19919d3-6dbf-4ea1-922c-24539383873a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads the qrel relevance assessment result file and returns a dictionary whose\n",
    "- key: QueryID\n",
    "- value: list of DocId that are relevant(relevance == \"1\") to the QueryID according to the relevance judgments.  \n",
    "\"\"\"\n",
    "def retrieve_relevance_assessment(qrel_file):\n",
    "    temp_relevance_scores = {}\n",
    "\n",
    "    with open(qrel_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            query_id, _, doc_id, relevance = line.strip().split()\n",
    "            relevance = int(relevance)  # convert relevance to int for calculations\n",
    "\n",
    "            key = (query_id, doc_id)\n",
    "\n",
    "            # if the key is already in the dictionary, update the sum and count\n",
    "            if key in temp_relevance_scores:\n",
    "                temp_relevance_scores[key][\"sum\"] += relevance\n",
    "                temp_relevance_scores[key][\"count\"] += 1\n",
    "            else:\n",
    "                # if not, initialize the sum and count\n",
    "                temp_relevance_scores[key] = {\"sum\": relevance, \"count\": 1}\n",
    "\n",
    "    #  as we have multiple scores for the same query_id and doc_id pair\n",
    "    for (query_id, doc_id), scores in temp_relevance_scores.items():\n",
    "        # calculate the average relevance score\n",
    "        average_relevance = scores[\"sum\"] / scores[\"count\"]\n",
    "        # check if the average relevance score >= 1\n",
    "        if average_relevance >= 1:\n",
    "            if query_id in rele_assess_dict:\n",
    "                rele_assess_dict[query_id].append(doc_id)\n",
    "            else:\n",
    "                rele_assess_dict[query_id] = [doc_id]\n",
    "    return rele_assess_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfb26b-01ed-40be-b663-28b1e1f84646",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Methods: Printing Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9fc5ff4d-c53f-4da5-add6-89abb19687de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prints the mean values of sublists in a structured and aligned format.\n",
    "\n",
    "Parameters:\n",
    "- lst (list): A list of lists where each sublist contains numeric values.\n",
    "- desc (str): A descriptive label for the values being printed.\n",
    "- kVals (list): An optional list of indices to specify which sublists to calculate the mean for.\n",
    "- qid (str): An optional identifier to include in the printed output.\n",
    "\"\"\"\n",
    "def print_mean_vals(lst, desc, kVals=[], qid=\"\"):\n",
    "    print(desc)\n",
    "    \n",
    "    if not kVals:\n",
    "        # if kVals is empty, calculate and print the mean for the entire list.\n",
    "        mean_val = math.fsum(lst) / len(lst)\n",
    "        print(\"For all rel docs: {:.4f}\".format(mean_val))\n",
    "    else:\n",
    "        # calcualte and print the mean for each specified sublist.\n",
    "        for k in kVals:\n",
    "            mean_val = math.fsum(lst[k]) / len(lst[k])\n",
    "            # If qid is provided, include it in the print statement.\n",
    "            if qid != \"\":\n",
    "                print(\"  At {:5} docs for {}: {:.4f}\".format(k, qid, mean_val))\n",
    "            else:\n",
    "                # use ':5': ensures that the number is right-aligned in a space of 5 chars.\n",
    "                print(\"  At {:5} docs: {:.4f}\".format(k, mean_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e4a72c56-93f8-4aa9-bb68-3ac69e119be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metric(description, value, query_id):\n",
    "    print(f\"{description} for {query_id}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53532f46-8a65-4207-a05b-afd0e04167ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Methods: Calcualte Precisions & Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "353d2f4a-36f0-4445-b709-1fc715006a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the precision\n",
    "\"\"\"\n",
    "def calculate_precision(relevant_number, rank):\n",
    "    return relevant_number / rank if rank else 0   # avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ff78c541-d644-47f3-a95e-7c424031109d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the recall\n",
    "The fraction of relevant documents retrieved by the system.\n",
    "\"\"\"\n",
    "def calculate_recall(relevant_number, total_relevant):\n",
    "    return relevant_number / total_relevant if total_relevant else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "65d123f9-6eab-429c-9655-c056ec3d8a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the F1-Measure\n",
    "- combine precision and recall into a single value\n",
    "- harmonic mean: F= 2PR/(P+R)\n",
    "\"\"\"\n",
    "def calculate_f1(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall) if precision + recall else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "92b07251-e1bf-4ad9-898a-5d88cc3c8d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the normalized Discounted Cumulative Gain (nDCG)\n",
    "- normalize DCG by iDCG to find nDCG:  nDCG = DCG/IDCG\n",
    "\"\"\"\n",
    "def calculate_nDCG(relevance_scores):\n",
    "    \n",
    "    # get DCG using the relevance scores\n",
    "    dcg = sum(\n",
    "        score / math.log(1.0 + rank)\n",
    "        for rank, score in enumerate(relevance_scores, start=1)\n",
    "    )\n",
    "    \n",
    "    # get the IDCG using the sorted relevance scores\n",
    "    sorted_scores = sorted(relevance_scores, reverse=True)\n",
    "    idcg = sum(\n",
    "        score / math.log(1.0 + rank)\n",
    "        for rank, score in enumerate(sorted_scores, start=1)\n",
    "    )\n",
    "    \n",
    "    return dcg / idcg if idcg else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ac66f40f-e977-4300-927c-d74a18153686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate R-Precision\n",
    "- rp: # of relevant doc that have been retrieved.\n",
    "- total_relevant: # of relevant doc that exist for the given query.\n",
    "\"\"\"\n",
    "def calculate_r_precision(rp, total_relevant):\n",
    "    return rp / total_relevant if total_relevant else 0\n",
    "\n",
    "\n",
    "# def calculate_r_precision(retrieved_relevant, total_relevant_docs):\n",
    "#     return retrieved_relevant / total_relevant_docs if total_relevant_docs else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2f12ef18-ffd1-4f65-993c-ed9c242c1770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_metric(metric_dict, key, value):\n",
    "    if key in metric_dict:\n",
    "        metric_dict[key].append(value)\n",
    "    else:\n",
    "        metric_dict[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5ab6e684-5210-488e-8d41-b5950dcc44cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper methods to write each docid per queryid's result to a file\n",
    "\"\"\"\n",
    "def write_details(f, query_id, document, rank, is_relevant, precision, recall):\n",
    "    f.write(f\"{query_id} {document} {rank} {is_relevant} {precision:.4f} {recall:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2a0104ec-3a4a-4d75-8857-9edf12194db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Calculate and print evaluation metrics for retrieved results.\n",
    "Computes various evaluation metrics, such as Average Precision (AP),R-Precision (RP), and Normalized Discounted Cumulative Gain (nDCG) for a set of\n",
    "queries and their retrieved results. Precision, Recall, and F1 score are calculated at predefined rank thresholds (k-values). \n",
    "It also provides the option to print detailed metrics for each individual query.\n",
    "\"\"\"\n",
    "def calculate_metrics(query_results, option):\n",
    "    \n",
    "    # initialize lists and dictionaries to store  metrics\n",
    "    k_vals = [5, 10, 20, 50, 100]\n",
    "    AP, RP, NDCG = [], [], [] # lists to store Average Precision, R-Precision, and nDCG for each query\n",
    "    P, R, F1 = {}, {}, {}     # dicts to store Precision, Recall, and F1 scores at different k values across all queries\n",
    "    \n",
    "    with open(\"details.txt\", \"w\") as f:\n",
    "        \n",
    "        for query_id, results in query_results.items():\n",
    "            relevant_docs = rele_assess_dict.get(query_id, [])\n",
    "            if not relevant_docs:\n",
    "                continue\n",
    "\n",
    "            # Initialize variables for calculating metrics\n",
    "            relevance_scores = []\n",
    "            p_temp, r_temp, f1_temp = {}, {}, {}\n",
    "            p_sum, relevant_count, rp = 0, 0, 0\n",
    "            \n",
    "            # Iterate over each document and its rank in the results\n",
    "            for rank, document in enumerate(results, start=1):\n",
    "                is_relevant = document in relevant_docs                        # check if the current document is relevant\n",
    "                relevant_count += is_relevant                                  # increment count if the document is relevant\n",
    "                rp = relevant_count if rank <= len(relevant_docs) else rp      # calculate R-Precision based on relevant count\n",
    "                precision = calculate_precision(relevant_count, rank)          # calculate precision at current rank\n",
    "                recall = calculate_recall(relevant_count, len(relevant_docs))  # calculate recall at current rank\n",
    "                p_sum += precision * is_relevant                               # add to precision sum for Average Precision calculation\n",
    "                \n",
    "                # update metrics at predefined k values\n",
    "                if rank in k_vals:\n",
    "                    update_metric(P, rank, precision)\n",
    "                    update_metric(p_temp, rank, precision)\n",
    "                    f1 = calculate_f1(precision, recall)\n",
    "                    update_metric(F1, rank, f1)\n",
    "                    update_metric(f1_temp, rank, f1)\n",
    "                    update_metric(R, rank, recall)\n",
    "                    update_metric(r_temp, rank, recall)\n",
    "                    \n",
    "                # append relevance score for nDCG calculation\n",
    "                relevance_scores.append(is_relevant)\n",
    "                # write detailed results to file => laster used in the plotting\n",
    "                write_details(\n",
    "                    f, query_id, document, rank, is_relevant, precision, recall\n",
    "                )\n",
    "                \n",
    "            # calculate and store R-Precision, nDCG, and Average Precision for the current query\n",
    "            r_precision = calculate_r_precision(rp, len(relevant_docs))\n",
    "            RP.append(r_precision)\n",
    "            \n",
    "            ndcg = calculate_nDCG(relevance_scores)\n",
    "            NDCG.append(ndcg)\n",
    "            \n",
    "            avg_precision = p_sum / len(relevant_docs) if relevant_docs else 0\n",
    "            AP.append(avg_precision)\n",
    "            \n",
    "            # print the metrics detail for the current query if option 1 is selected\n",
    "            if option == 1:\n",
    "                print_metric(\"Average Precision\", avg_precision, query_id)\n",
    "                print_metric(\"R-Precision\", r_precision, query_id)\n",
    "                print_metric(\"nDCG\", ndcg, query_id)\n",
    "                print(\"\\nPrecision@ Values\")\n",
    "                print_mean_vals(p_temp, \"Mean Precision@\", k_vals, query_id)\n",
    "                print(\"\\nRecall@ Values\")\n",
    "                print_mean_vals(r_temp, \"Mean Recall@\", k_vals, query_id)\n",
    "                print(\"\\nF1@ Values\")\n",
    "                print_mean_vals(f1_temp, \"Mean F1@\", k_vals, query_id)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "    # print summary for all queries\n",
    "    print(\"========================================\")\n",
    "    print(\"============== Summary =================\")\n",
    "    print(\"========================================\")\n",
    "    print_mean_vals(AP, \"\\nAverage Precision:\")\n",
    "    print_mean_vals(RP, \"\\nR-Precision:\")\n",
    "    print_mean_vals(NDCG, \"\\nnDCG:\")\n",
    "    # print(\"\\n========= Precision@k =========\")\n",
    "    print_mean_vals(P, \"\\n========= Precision@k =========\", k_vals)\n",
    "    # print(\"\\n========= Recall@k =========\")\n",
    "    print_mean_vals(R, \"\\n=========== Recall@k ==========\", k_vals)\n",
    "    # print(\"\\n========= F1@k =========\")\n",
    "    print_mean_vals(F1, \"\\n============ F1@k ============\", k_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50856424-14f5-41bb-ba89-635a50781344",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Method: Main Run for Designed Trec-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e56b21c-f74d-47b6-abe2-f657572c6a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory\n",
    "PATH_SCRIPT = os.path.abspath(cwd) \n",
    "PATH_DIR_EVAL = os.path.join(PATH_SCRIPT, 'eval_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f6f40a17-ea5e-4cb7-b3c9-32bdf43f36f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_eval():\n",
    "    cmd = input(\"Command: \")\n",
    "    cmd_params = cmd.split(\" \")\n",
    "    \n",
    "    if len(cmd_params) == 4:\n",
    "\n",
    "        PATH_QRELS_FILE = os.path.join(PATH_DIR_EVAL, f'{cmd_params[2]}')\n",
    "        PATH_QUERY_RESULT_FILE =os.path.join(PATH_DIR_EVAL, f'{cmd_params[3]}')\n",
    "\n",
    "        retrieve_relevance_assessment(PATH_QRELS_FILE)\n",
    "        query_results = retrieve_query_results(PATH_QUERY_RESULT_FILE)\n",
    "        calculate_metrics(query_results, 1)\n",
    "        \n",
    "    else:\n",
    "        PATH_QRELS_FILE = os.path.join(PATH_DIR_EVAL, f'{cmd_params[1]}')\n",
    "        PATH_QUERY_RESULT_FILE =os.path.join(PATH_DIR_EVAL, f'{cmd_params[2]}')\n",
    "        \n",
    "        retrieve_relevance_assessment(PATH_QRELS_FILE)\n",
    "        query_results = retrieve_query_results(PATH_QUERY_RESULT_FILE)\n",
    "        \n",
    "        calculate_metrics(query_results, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5762417-c1d0-4054-a57e-12cfe22c6b27",
   "metadata": {},
   "source": [
    "### 5. Call Trec Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8cca1-a175-4d5e-9041-4252ff6495d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a. For HW1 QREL file & Ranklist Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1fd0efec-c15d-4cb8-b4dd-ef41ca898d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trec_eval qrels.adhoc.51-100.AP89.txt 1_query_result_es_builtin.txt\n",
    "# trec_eval qrels.adhoc.51-100.AP89.txt 2_query_result_okapi_tf.txt\n",
    "# trec_eval qrels.adhoc.51-100.AP89.txt 3_query_result_tfidf.txt\n",
    "# trec_eval qrels.adhoc.51-100.AP89.txt 4_query_result_okapi_bm25.txt\n",
    "# trec_eval qrels.adhoc.51-100.AP89.txt 5_query_result_lm_laplace.txt\n",
    "# trec_eval qrels.adhoc.51-100.AP89.txt 6_query_result_lm_jm.txt\n",
    "\n",
    "# trec_eval [-q] qrels.adhoc.51-100.AP89.txt 2_query_result_okapi_tf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "eeca4759-1ed5-40cd-bfb3-1fb241ab6a54",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:  trec_eval qrels.adhoc.51-100.AP89.txt 2_query_result_okapi_tf.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "============== Summary =================\n",
      "========================================\n",
      "\n",
      "Average Precision:\n",
      "For all rel docs: 0.2316\n",
      "\n",
      "R-Precision:\n",
      "For all rel docs: 0.2573\n",
      "\n",
      "nDCG:\n",
      "For all rel docs: 0.6125\n",
      "\n",
      "========= Precision@k =========\n",
      "  At     5 docs: 0.4480\n",
      "  At    10 docs: 0.3920\n",
      "  At    20 docs: 0.3420\n",
      "  At    50 docs: 0.2736\n",
      "  At   100 docs: 0.1996\n",
      "\n",
      "=========== Recall@k ==========\n",
      "  At     5 docs: 0.0593\n",
      "  At    10 docs: 0.0951\n",
      "  At    20 docs: 0.1489\n",
      "  At    50 docs: 0.2464\n",
      "  At   100 docs: 0.3510\n",
      "\n",
      "============ F1@k ============\n",
      "  At     5 docs: 0.0933\n",
      "  At    10 docs: 0.1290\n",
      "  At    20 docs: 0.1682\n",
      "  At    50 docs: 0.2100\n",
      "  At   100 docs: 0.2069\n"
     ]
    }
   ],
   "source": [
    "rele_assess_dict = {}\n",
    "run_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da939ff-f2af-4707-98e7-5c7b303f603a",
   "metadata": {},
   "source": [
    "#### b. For QREL & Ranklist Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8054eb8-e5fc-451f-9958-6272539e2ec6",
   "metadata": {},
   "source": [
    "##### Queries: Evaluation Rank list -- use elasticsearch built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cac98dbd-d569-473a-a449-4015124595b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = {\n",
    "    \"152901\": \"West African Ebola epidemic\",\n",
    "    \"152902\": \"H1N1 Swine Flu pandemic\",\n",
    "    \"152903\": \"COVID 19\",\n",
    "}\n",
    "\n",
    "# modify the queries => tokenized\n",
    "queries_map = {qid: word_tokenize(query) for qid, query in QUERIES.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07da8371-ffbd-4a98-aa12-f0a1d9d00df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "INDEX_NAME = 'crawler' \n",
    "CLOUD_ID = \"6200:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRiZTllZjE5NDRkNTg0MDE3YTU0NDg0MzcwYjk5MjQzMSQ2Zjg1ODJhNWRjMGY0NDBhODU1Njk1MDQ4NzMyNmU2Yg==\"                  \n",
    "es = Elasticsearch(request_timeout = 10000, \n",
    "                    cloud_id = CLOUD_ID,\n",
    "                    http_auth = ('elastic', 'fwOhKti7myB3PKFHQavQBhcr'))\n",
    "\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "64c0b965-c44f-4acd-bb19-2e27d49d33df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def es_search(q):\n",
    "    res_es_search = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    'content': ' '.join(q)\n",
    "                }\n",
    "            },\n",
    "            'size': 1000\n",
    "        }\n",
    "    )\n",
    "    return res_es_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "db70bcf3-d278-41c7-98ae-275d871cd211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Use 'es_search' method to get the relavant document for each of the queries\n",
    "2. Write the top 1000 documents - for es-model the docs are already in the sorted order based on the score \n",
    "\"\"\"\n",
    "PATH_OUTPUT_ES_BUILT_IN = os.path.join(PATH_DIR_EVAL, 'ranklist.txt')\n",
    "\n",
    "es_result_dict = {} #  a dictionary of (a list of tuples)\n",
    "\n",
    "with open(PATH_OUTPUT_ES_BUILT_IN, \"w\") as output_file:\n",
    "    for q_num, q_tokens in queries_map.items():\n",
    "        q_results = []\n",
    "        output = es_search(q_tokens)\n",
    "        hits = output['hits']['hits']\n",
    "        for rank, hit in enumerate(hits, start=1):\n",
    "            docno = hit['_id']\n",
    "            score = hit['_score']\n",
    "            output_line = f\"{q_num} Q0 {docno} {rank} {score} Exp\"\n",
    "            output_file.write(output_line + \"\\n\")\n",
    "            q_results.append((docno, score))  \n",
    "        es_result_dict[q_num] = q_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf4f90-61c8-4f5c-8bec-b256323bf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trec_eval [-q] qrel.txt ranklist.txt\n",
    "# trec_eval qrel.txt ranklist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b2340fc6-723e-4a37-902d-2007ecc2d5ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:  trec_eval qrel.txt ranklist.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "============== Summary =================\n",
      "========================================\n",
      "\n",
      "Average Precision:\n",
      "For all rel docs: 0.8966\n",
      "\n",
      "R-Precision:\n",
      "For all rel docs: 0.8765\n",
      "\n",
      "nDCG:\n",
      "For all rel docs: 0.9871\n",
      "\n",
      "========= Precision@k =========\n",
      "  At     5 docs: 1.0000\n",
      "  At    10 docs: 1.0000\n",
      "  At    20 docs: 1.0000\n",
      "  At    50 docs: 0.9867\n",
      "  At   100 docs: 0.9900\n",
      "\n",
      "=========== Recall@k ==========\n",
      "  At     5 docs: 0.0271\n",
      "  At    10 docs: 0.0543\n",
      "  At    20 docs: 0.1085\n",
      "  At    50 docs: 0.2673\n",
      "  At   100 docs: 0.5370\n",
      "\n",
      "============ F1@k ============\n",
      "  At     5 docs: 0.0528\n",
      "  At    10 docs: 0.1029\n",
      "  At    20 docs: 0.1957\n",
      "  At    50 docs: 0.4203\n",
      "  At   100 docs: 0.6953\n"
     ]
    }
   ],
   "source": [
    "rele_assess_dict = {}\n",
    "run_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a89f6a-2a69-40b3-aae6-e88e049fd9f4",
   "metadata": {},
   "source": [
    "##  Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "13a10433-8e10-4027-bb29-a3fde7208926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_precision_recall(details_file):\n",
    "    query_dict = {}  # dict to hold query_id: {doc_id: (precision, recall), ...}\n",
    "\n",
    "    with open(details_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.split() \n",
    "            if len(parts) == 6: \n",
    "                query_id, doc_id, rank, is_relevant, precision, recall = parts\n",
    "                precision = float(precision)  \n",
    "                recall = float(recall) \n",
    "\n",
    "                if query_id not in query_dict:\n",
    "                    query_dict[query_id] = {}\n",
    "\n",
    "                # add the precision and recall to the dictionary using doc_id as the key\n",
    "                query_dict[query_id][doc_id] = (precision, recall)\n",
    "\n",
    "    return query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5575d3c9-510f-434e-b353-c621a43965f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_curves(query_id, interpolated_prec_list, rec_list):\n",
    "    plt.figure()\n",
    "    # plt.plot(rec_list, interpolated_prec_list, marker='o', linewidth=0.1)\n",
    "    plt.plot(rec_list, interpolated_prec_list, marker='o', linewidth=0.1, markersize=3)\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.title(f\"Precision-Recall Curve for Query {query_id}\")\n",
    "    plt.axis([0, 1, 0, 1])  # use a list for axis limits\n",
    "    plt.grid(True)  # add grid\n",
    "    plt.savefig(f\"{query_id}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_plot_from_dict(query_id, query_dict):\n",
    "\n",
    "    if query_id in query_dict:\n",
    "        # extract lists of precision and recall from the query_dict\n",
    "        precision_list, recall_list = zip(*query_dict[query_id].values())\n",
    "\n",
    "        # sort the pairs by recall \n",
    "        paired_list = sorted(zip(recall_list, precision_list))\n",
    "        sorted_recall_list, sorted_precision_list = zip(*paired_list)\n",
    "\n",
    "        # plot curves using the sorted lists\n",
    "        plot_curves(query_id, sorted_precision_list, sorted_recall_list)\n",
    "    else:\n",
    "        print(f\"No data found for query ID {query_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "19a3a529-630e-45ba-b838-3bb38cba2c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "details_file = 'details.txt'\n",
    "query_precision_recall_dict = extract_precision_recall(details_file)\n",
    "generate_plot_from_dict('152901', query_precision_recall_dict)\n",
    "generate_plot_from_dict('152902', query_precision_recall_dict)\n",
    "generate_plot_from_dict('152903', query_precision_recall_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
